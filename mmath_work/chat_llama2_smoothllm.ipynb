{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luca-michaelides/MT25_Python/blob/main/chat_llama2_smoothllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5300fa",
      "metadata": {
        "id": "ea5300fa"
      },
      "source": [
        "## Initial test with Llama2 LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SA5H2Zt6H5WV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379,
          "referenced_widgets": [
            "2951b21c969e49688fa56e95d122127c",
            "67b755f03879460094d79cc90905098f",
            "68e05cac315d4d36984c7162c5e91507",
            "22e94cd382d3412f8398b533f0e898da",
            "80e0e0d537b542eca85cc4617b6caf58",
            "8fa308e5b735478192ff10ff78fc845a",
            "fca47dcc987b4037b2c26c89b27fbfdd",
            "e86c6a2b7b79427681d17a087ad7a87c",
            "eff4d9ebabc6423588a409782017d1bd",
            "b8fec5bb74d6494189a0f0086bdf8e94",
            "88c58668c7c44f2d9d29a08c19f4adb6"
          ]
        },
        "collapsed": true,
        "id": "SA5H2Zt6H5WV",
        "outputId": "411e5b68-379b-4483-e225-9a4561e38e10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLoading tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 4-bit quantized model (this may take 60-120s)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2951b21c969e49688fa56e95d122127c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded. Sample param device: cuda:0\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Generation time: 2.37s\n",
            "Decoded output:\n",
            " [INST] <<SYS>>\n",
            "You are a helpful assistant.\n",
            "<</SYS>>\n",
            "\n",
            "Say hello briefly. [/INST]  Hello! It's nice to meet you! How can I assist you today?\n",
            "\n",
            "--- nvidia-smi ---\n",
            "name, memory.total [MiB], memory.used [MiB]\n",
            "Tesla T4, 15360 MiB, 6568 MiB\n"
          ]
        }
      ],
      "source": [
        "import os, gc, time, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "try:\n",
        "    del model\n",
        "except NameError:\n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_token)\n",
        "\n",
        "print(\"Loading 4-bit quantized model (this may take 60-120s)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded. Sample param device:\", next(model.parameters()).device)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "prompt = \"<s>[INST] <<SYS>>\\nYou are a helpful assistant.\\n<</SYS>>\\n\\nSay hello briefly. [/INST]\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs.pop(\"token_type_ids\", None)\n",
        "device = next(model.parameters()).device\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "t0 = time.time()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=24,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "t1 = time.time()\n",
        "print(\"Generation time: %.2fs\" % (t1 - t0))\n",
        "print(\"Decoded output:\\n\", tokenizer.decode(out[0], skip_special_tokens=True))\n",
        "\n",
        "print(\"\\n--- nvidia-smi ---\")\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HNq_7M0nDD_e",
      "metadata": {
        "id": "HNq_7M0nDD_e"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "def format_llama2_chat_prompt(user_message, system_message=\"You are a helpful assistant.\"):\n",
        "    return (\n",
        "        \"<s>[INST] <<SYS>>\\n\"\n",
        "        f\"{system_message}\\n\"\n",
        "        \"<</SYS>>\\n\\n\"\n",
        "        f\"{user_message} [/INST]\"\n",
        "    )\n",
        "\n",
        "import time, torch\n",
        "\n",
        "# find device where most params live (works for sharded models)\n",
        "_model_device = next(model.parameters()).device\n",
        "\n",
        "def chat(user_input, max_tokens=64, temperature=0.1, do_sample=None):\n",
        "    if do_sample is None:\n",
        "        do_sample = (temperature > 0)\n",
        "\n",
        "    prompt = format_llama2_chat_prompt(user_input)  # your formatting function\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs.pop(\"token_type_ids\", None)\n",
        "\n",
        "    # move inputs to model device (handles sharded / auto device_map)\n",
        "    inputs = {k: v.to(_model_device) for k, v in inputs.items()}\n",
        "\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=do_sample,\n",
        "            temperature=temperature if do_sample else 0.0,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True\n",
        "        )\n",
        "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "    dt = time.time() - t0\n",
        "\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    # The model returns prompt+response; return only the part after the [/INST] marker if you prefer:\n",
        "    # split on the closing tag and keep the tail:\n",
        "    tail = decoded.split(\"[/INST]\")[-1].strip()\n",
        "    print(tail)\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h7Bl-D0SDpeT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Bl-D0SDpeT",
        "outputId": "e16a6ce0-9be7-47cf-e513-e0a95ed21807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nt_aJBasOJJB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt_aJBasOJJB",
        "outputId": "0f165616-288f-4f26-f889-b97f94b232ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Copy of chat_llama2_test (1).ipynb'  'Copy of chat_llama2_test.ipynb'\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Colab\\ Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E6TQejOIOvhf",
      "metadata": {
        "id": "E6TQejOIOvhf"
      },
      "outputs": [],
      "source": [
        "import nbformat, shutil, os\n",
        "\n",
        "NOTEBOOK_PATH = \"/content/drive/MyDrive/Colab\\ Notebooks/chat_llama2_smoothllm.ipynb\"\n",
        "BACKUP_PATH = NOTEBOOK_PATH + \".bak\"\n",
        "\n",
        "if not os.path.exists(NOTEBOOK_PATH):\n",
        "    raise FileNotFoundError(f\"Notebook not found: {NOTEBOOK_PATH}\")\n",
        "\n",
        "shutil.copy2(NOTEBOOK_PATH, BACKUP_PATH)\n",
        "print(\"Backup created:\", BACKUP_PATH)\n",
        "\n",
        "nb = nbformat.read(NOTEBOOK_PATH, as_version=nbformat.NO_CONVERT)\n",
        "\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    nb[\"metadata\"].pop(\"widgets\", None)\n",
        "    nbformat.write(nb, NOTEBOOK_PATH)\n",
        "    print(\"Removed metadata.widgets and saved:\", NOTEBOOK_PATH)\n",
        "else:\n",
        "    print(\"No metadata.widgets found — nothing to remove.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}